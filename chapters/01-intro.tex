\chapter{Introduction}
\label{sec:intro}

Fifty years after its initial proposal by Edgar F. Codd~\cite{DBLP:journals/cacm/Codd70},
 the relational model has cemented its status as the de-facto data model 
 in nearly all modern databases.
It provides {\em data independence}, 
 which has allowed data systems to scale to unprecedented sizes
 while guaranteeing performance and correctness.
The success of the relational model is witnessed 
 by the popularity of SQL, 
 uniquitous in computing systems 
 from smartphones to data centers.

 Nevertheless, the capabilities of traditional databases are struggling 
  to meet the demands of modern data analytics.
Today's data analytics involve new kinds of data, 
 as well as new kinds of computation over such data. 
For example, machine learning workloads involve linear algebra operations
 running over data stored as matrices. 
For another example, graph analytics require iterative algorithms
 over graph data.
Running these workloads in existing relational databases 
 is both cumbersome and slow:
 {\em SQL is a poor language for expressing the computation,
 and the systems are not optimized for such workloads}.

This dissertation is motivated by the question: 
{\em How can we renew relational database systems 
 to support modern data analytics?}
Towards that end, we propose a new language foundation
 for writing relational programs;
 a new algorithm for the relational join;
 an optimizer for linear algebra through relational algebra, 
 and optimization techniques for recursive queries.
Together, these four ingredients make up 
 the basis for a new generation of relational systems
 that are more expressive and more efficient.
Using these systems,
 the analyst may author entire application programs
 instead of simple queries {\em relationally},
 entering the era of {\em relational programming}.

\section{Motivation and Contributions}
\label{sec:intro:motivation}

% For fifty years, the relational data model has been the main choice for
% representing, modeling, and processing data.  
The main query language for relational data, SQL, 
 is found today in a wide range of applications and
 devices. 
%  from smart phones, to database servers, to distributed
%  cloud-based clusters.
The reason for its success is the {\em data
  independence principle}, which separates the declarative model from
the physical implementation~\cite{DBLP:journals/cacm/Codd70}, and
enables advanced implementation techniques, such as cost-based
optimizations, indices, materialized views, incremental view
maintenance, parallel evaluation, and many others, while keeping the
same simple, declarative interface to the data.

But data scientists today often need to perform tasks that require
iteration over the data,
 such as gradient descent, clustering, page-rank, network centrality, and inference
 in knowledge graphs.
While SQL has introduced recursion since 1999
(through Common Table Expressions, CTE), it has many cumbersome
limitations and is little used in practice~\cite{frankmcsherry-2022}.

The need to support recursion in a declarative language led to the
development of Datalog in the mid 80s~\cite{DBLP:conf/pods/Vianu21}.
Datalog adds recursion to the relational query language, yet enjoys several elegant
properties: it has a simple, declarative semantics; its na\"ive
bottom-up evaluation algorithm always terminates; and it admits a few
powerful optimizations, such as semi-na\"ive evaluation and magic set
rewriting.  Datalog has been extensively studied in the literature;
see~\cite{DBLP:journals/ftdb/GreenHLZ13} for a survey
and~\cite{DBLP:books/mc/18/MaierTKW18,DBLP:conf/pods/Vianu21} for historical notes.
We briefly review the semantics and execution of Datalog in
 Chapter~\ref{chap:background}.

However, Datalog is not the answer to modern needs, because it only
supports monotone queries over sets.  Most tasks in data science today
require the interleaving of recursion and aggregate computation.
Aggregates are not monotone under set inclusion, and therefore they
are not supported by the framework of pure Datalog.  Neither SQL'99
nor popular open-source Datalog systems like
Souffl\'e~\cite{DBLP:conf/cav/JordanSS16} allow recursive queries to
have aggregates.  While several proposals have been made to extend
Datalog with
aggregation~\cite{DBLP:conf/pods/GangulyGZ91,DBLP:conf/pods/RossS92,DBLP:journals/jcss/GangulyGZ95,DBLP:journals/vldb/MazuranSZ13,DBLP:conf/icde/ShkapskyYZ15,DBLP:conf/sigmod/ShkapskyYICCZ16,DBLP:conf/amw/ZanioloYDI16,DBLP:journals/tplp/ZanioloYDSCI17,DBLP:conf/amw/ZanioloYIDSC18,DBLP:journals/tplp/CondieDISYZ18,DBLP:conf/sigmod/0001WMSYDZ19,DBLP:journals/corr/abs-1910-08888,DBLP:journals/corr/abs-1909-08249,DBLP:journals/debu/ZanioloD0LL021},
these extensions are at odds with the elegant properties of Datalog
and have not been adopted by either Datalog systems or SQL engines.

\textbf{
The first contribution of this dissertation is a foundation for a query language that
supports both recursion and aggregation.}  
Our language, called \datalogo, is a generalization of Datalog. 
Intuitively, Datalog works over sets, which are functions from tuples to booleans.
\datalogo instead works over functions from tuples to values from a semiring.
For example, setting the semiring to be the reals allows us to define recursive 
 tensor programs in \datalogo,
 and the using the min-plus tropical semiring $(\N \cup \set{\infty}, \min, +, \infty, 0)$
 lets us compute shortest paths over weighted graphs.
\datalogo retains many of the elegant properties of Datalog,
 while extending its expressiveness to support aggregation.

To evaluate any relational program, 
 including \datalogo programs,
 classic Datalog programs, 
 or even SQL queries without recursion, 
 a key operation is the {\em relational join}.
The join allows us to combine data from different sources, 
 as well as compose computation from different programs.
Most mainstream databases today evaluate a query 
 by joining two relations at a time.
In other words, they implement {\em binary join} algorithms.
Over the last decade, worst-case optimal join (\WCOJ)~\cite{
  DBLP:conf/pods/NgoPRR12,
  DBLP:conf/icdt/Veldhuizen14, 
  DBLP:journals/sigmod/NgoRR13, 
  DBLP:conf/pods/000118}
 has emerged as
 a breakthrough in the design of efficient join algorithms.  
It can be
asymptotically faster than traditional binary joins, all the while
remaining simple to understand and
implement~\cite{DBLP:journals/sigmod/NgoRR13}.  
% \WCOJ has opened up a flourishing field of research, leading to both theoretical
% results~\cite{DBLP:journals/sigmod/NgoRR13,DBLP:conf/pods/Khamis0S17}
% and practical
% implementations~\cite{DBLP:conf/icdt/Veldhuizen14,DBLP:journals/tods/AbergerLTNOR17,DBLP:journals/pvldb/FreitagBSKN20, DBLP:journals/pvldb/MhedhbiS19}.
However, traditional binary join algorithms have benefited from 
  decades of research and engineering.
Techniques like column-oriented layout, vectorization, 
  and query optimization
  have contributed compounding constant-factor speedups,
  making it challenging for \WCOJ to be competitive in practice.

\textbf{
The second contribution of this dissertation is a new join algorithm,
  called \FJ, that unifies \WCOJ and binary join.}
We propose several new techniques to make \FJ outperform
both binary join and \WCOJ:
\begin{enumerate}
\item An algorithm to convert any binary join plan to a \FJ
  plan that runs as fast or faster.
\item A new data structure called
\COLT (for \emph{Column-Oriented Lazy Trie}), adapting the classic
column-oriented layout to improve the trie data structure used in
\WCOJ.
\item A vectorized execution algorithm for \FJ.
\end{enumerate}

While a faster join algorithm improves the performance of 
 relational programs at the operator level,
 modern data analytics presents abundant opportunities
 for higher-level optimizations.
For example, machine learning pipelines heavily rely on 
 linear algebra operations,
 and the order to carry out these operations
 can have a significant impact on performance.
Consider the Linear Algebra (LA) expression $sum((X-UV^T)^2)$ which
defines a typical loss function for approximating a matrix $X$ with a
low-rank matrix $UV^T$. Here, $sum()$ computes the sum of all matrix
entries in its argument, and $A^2$ squares the matrix $A$
element-wise. Suppose $X$ is a sparse, 1M x 500k matrix, and suppose
$U$ and $V$ are dense vectors of dimensions 1M and 500k respectively.
Thus, $UV^T$ is a rank 1 matrix of size 1M x 500k, and computing it
naively requires 0.5 trillion multiplications, plus memory allocation.
Fortunately, the expression is equivalent to
$sum(X^2) - 2U^TXV + U^TU * V^TV$.  Here $U^TXV$ is a scalar that can
be computed efficiently by taking advantage of the sparsity of $X$,
and, similarly, $U^TU$ and $V^TV$ are scalar values requiring only 1M
and 500k multiplications respectively.

Optimization opportunities like this are ubiquitous in machine
learning programs. State-of-the-art optimizing compilers such as
SystemML~\cite{DBLP:reference/bdt/Boehm19},
OptiML\cite{DBLP:conf/icml/SujeethLBRCWAOO11}, and
Cumulon\cite{DBLP:conf/sigmod/HuangB013} commonly implement syntactic
rewrite rules that exploit the algebraic properties of the LA
expressions. For example, SystemML includes a rule that rewrites the
preceding example to a specialized operator\footnote{See the
  \href{https://systemml.apache.org/docs/0.12.0/engine-dev-guide.html}{SystemML
    Engine Developer Guide} for details on the weighted-square loss
  operator \texttt{wsloss}. } to compute the result in a streaming
fashion.  However, such syntactic rules fail on the simplest
variations, for example SystemML fails to optimize $sum((X+UV^T)^2)$,
where we just replaced $-$ with $+$.  Moreover, rules may interact
with each other in complex ways.  In addition, complex ML programs
often have many common subexpressions (CSE) that further interact
with syntactic rules, for example the same expression $UV^T$ may occur
in multiple contexts, each requiring different optimization rules.

\textbf{The third contribution of this dissertation is \sys, a novel optimization approach for
complex linear algebra programs} that leverages relational algebra as
an intermediate representation to completely represent the search
space.
SPORES first transforms LA expressions into traditional
Relational Algebra (RA) expressions consisting of joins, unions 
and aggregates.  It then performs a cost-based optimizations on
the resulting Relational Algebra expressions, using only standard
identities in RA.  Finally, the resulting RA expression is converted
back to LA, and executed.

Beyond linear algebra, 
 the prevalence of recursion in modern workloads
 also calls for novel optimization techniques.
% Recall that supporting recursion and aggregation 
%  motivated us to design \datalogo.
Yet the optimization problem for recursive queries is little studied.
Although various data systems support recursion and iteration
 in one form or another, 
 most systems only optimize within each iteration.
The few systems that optimize across iterations
 apply limited techniques, like magic set optimization
 or semi-na\"ive evaluation,
 which are restricted to postitive queries.

For example, consider the following Datalog query, 
 computing the distance between all pairs of nodes in a graph: 
%
\begin{lstlisting}[mathescape=true]
path(X,Y,L) :- edge(X,Y,L).
path(X,Y,L) :- edge(X,Z,L$_1$), path(Z,Y,L$_2$), L=L$_1$+L$_2$.
dist(X,Y,D) :- D=min{ L | path(X,Y,L) }.
\end{lstlisting}
%
We will cover the semantics of Datalog in Chapter~\ref{chap:background},
 but intuitively explain the query here.
The first rule says that there is a path of length $L$ from $X$ to $Y$
 if there is an edge from $X$ to $Y$ of length $L$.
The second rule says that there is a path of length $L_1 + L_2$ from $X$ to $Y$
 if there is an edge from $X$ to $Z$ of length $L_1$,
 and a path of length $L_2$ from $Z$ to $Y$.
Finally, the last rule says that the distance from $X$ to $Y$ is the minimum
 length of all paths from $X$ to $Y$.
The program is intuitive yet inefficient:
 to find the shortest path between two nodes,
 it first computes {\em all} paths between the nodes.
A graph with cycles also contains an infinite number of paths,
 and the program will not terminate.
A different query, in our new \datalogo language, 
 computes the shortest paths more efficiently:
%
\begin{lstlisting}[mathescape=true]
path(X, Y) = min(edge(X, Y), min$_\texttt{Z}${ edge(X, Z) + path(Z, Y) })
\end{lstlisting}
%
Here we treat \lstinline|path| and \lstinline|edge| as functions
 from pairs of nodes to some length.
The program essentially follows the Bellman-Ford algorithm.
Initially we assign the distance of every pair to be the length of the edge between them. 
Then, in each iteration, we update the distance in a breadth-first manner,
 adding the length of an edge from a previously found distance. 
This new program is more efficient and terminates on cyclic graphs with postitive lengths.
It is natural to hope a query optimizer can rewrite the first program to the second,
 and perform similar optimization for other recursive queries.

\textbf{The fourth and final contribution of this dissertation is a new query optimization framework for
recursive queries.}
Our framework replaces a recursive program with
another, equivalent recursive program, whose body may be quite
different, and thus focuses on optimizing the recursive program as a
whole, not on optimizing its body in isolation; the latter can be done
separately, using standard query optimization techniques.  Our
optimization is based on a novel rewrite rule for recursive programs,
called the FGH-rule, which we implement using {\em program synthesis},
a technique developed in the programming languages and verification
communities. We introduce a new method for inferring loop invariants,
which extends the reach of the FGH-rule, and also show how to use
global constraints on the data for semantic optimizations using the
FGH-rule.  

\section{Chapter Organization \& Publications}

The rest of this dissertation is organized as follows:
\begin{itemize}
  \item Chapter~\ref{chap:related} covers related work to relational programming.
  \item Chapter~\ref{chap:background} reviews the basics of relational data processing.
  \item Chapter~\ref{chap:datalogo} introduces the \datalogo language as well as an algorithm to evaluate its programs. 
  \item Chapter~\ref{chap:free-join} presents the \FJ algorithm.
  \item Chapter~\ref{chapter:spores} presents the \sys system for optimizing linear algebra programs.
  \item Chapter~\ref{chapter:fgh} presents our approach to optimizing recursive queries using the FGH-rule.
  \item Chapter~\ref{chap:conclusions} concludes the dissertation and discusses future work.
\end{itemize}
%
Chapter~\ref{chap:datalogo} contains material from
 our PODS paper~\cite{DBLP:conf/pods/Khamis0PSW22}
 and SIGMOD Record article~\cite{DBLP:journals/sigmod/KhamisNPSW22}.
The full version of the PODS paper~\cite{DBLP:journals/sigmod/KhamisNPSW22} 
 contains additional proofs as well as detailed discussions on 
 several extensions to \datalogo.
Chapter~\ref{chap:free-join} is based on our SIGMOD paper~\cite{DBLP:journals/pacmmod/WangWS23}.
Chapter~\ref{chapter:spores} is based on our VLDB paper~\cite{DBLP:journals/pvldb/WangHSHL20},
 and another project using the same techniques to optimize deep learning inference was published at MLSys~\cite{DBLP:conf/mlsys/0008PWW0P21}.
 and Chapter~\ref{chapter:fgh} is based on our SIGMOD paper~\cite{DBLP:conf/sigmod/WangK0PS22}.
The equality saturation system used in projects throughout this dissertation
 was developed in a series of publications with my collaborators~\cite{DBLP:journals/pacmpl/NandiWZWSASGT21,DBLP:journals/pacmpl/ZhangWWT22,DBLP:journals/corr/abs-2304-04332,DBLP:journals/pacmpl/WillseyNWFTP21}.