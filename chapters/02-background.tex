\chapter{Background}
\label{chap:background}

This Chapter reviews basic concepts in relational query processing. 
We cover relations and (unions of) conjunctive queries in Section~\ref{sec:relations-and-conjunctive-queries},
 relational algebra with an emphasis on joins in Section~\ref{sec:join},
 and Datalog in Section~\ref{sec:datalog}.
By the end of this chapter, 
 the reader will have the knowledge necessary
 to build a very simple evaluator for Datalog.

\section{Relations and (Unions of) Conjunctive Queries}
\label{sec:relations-and-conjunctive-queries}

Let $D$ be a set of values, for example the set of natural numbers,
 or the set of ASCII strings.
A {\em tuple} over $D$ is of the form $(t_1, t_2, \ldots, t_k)$
 where each $t_i$ is a value in $D$,
 and $k$ is called the {\em arity} of the tuple.
A {\em relation} over $D$ is a set of tuples over $D$,
 each with the same arity which we call the arity of the relation.
Note that many popular database systems allow duplicate tuples in a relation;
 in other words they follow the so-called {\em bag semantics}.
We will focus on the set semantics in this section,
 and in Chapter~\ref{chap:datalogo} we will show how to
 support bag semantics by extending the relations
 with an algebraic structure called {\em semiring}.
Many databases also prescribe a {\em schema} for each relation.
We omit the schema because it is not necessary for the 
 conjunctive query notation that we follow in this thesis.
However, we will occasionally use SQL queries in our examples 
 to make them more readible to SQL programmers.

\begin{ex}
\label{ex:relation}
Let $D = \set{1, 2, \ldots, n}$ represent a set of $n$ vertices in a graph.
A binary relation $E$ over $D$ represents the (directed) edges in the graph.
Let $E \defeq \setof{(x, y)}{x, y \in D}$, then $E$ is the complete graph on $D$.
\end{ex}

Given a set of relations $R_1, R_2, \ldots, R_n$, 
 a {\em conjunctive query} (CQ) is of the form:
\[
  Q(\bm{X}) \cd R_1(\bm{X}_1), R_2(\bm{X}_2), \ldots, R_n(\bm{X}_n).
\]
where each $\bm{X}_i$ (and $\bm{X}$) is a tuple consisting of variables and constants.
$\bm{X}_i$ must have the same arity as $R_i$,
 and the arity of $\bm{X}$ is called the {\em arity} of the query.
Each $R_i({\bm X_i})$ (and $Q({\bm X})$) is called an {\em atom}.
% For simplicity, we assume no $\bm{x}_i$ (or $\bm{x}$) contains duplicate variables.
When the context is clear, 
 we will use $Q$ to refer to both the query and its output relation.
The query $Q$ is called {\em safe} if each variable in $\bm{X}$ also appears in some $\bm{X}_i$.
We will only consider safe queries in this thesis.
A variable only appearing in the body is called a {\em bound} variable.
Otherwise, it is called a {\em free} variable. 
$Q$ is called a {\em full} conjunctive query if it has no bound variables.
The conjunctive query computes in $Q$ the set 
 $\setof{\bm X}{\exists_{\bm X_\text{bound}} : \bigwedge_{i \in [1 \ldots n]} {\bm X_i} \in R_i}$
 where ${\bm X_\text{bound}}$ is the set of bound variables.

\begin{ex}
\label{ex:cq}
Using the relation $E$ from Example~\ref{ex:relation},
 the query $Q(X, Z) \cd E(X, Y), E(Y, Z).$
 computes all pairs of vertices $(X, Z)$ 
 such that there is a vertex $Y$ 
 connected to both $X$ and $Z$.
Formally, the set $\setof{(x, z)}{\exists y : (x, y) \in E \wedge (y, z) \in E}$.
In other words, it finds all paths of length 2 in the graph.
The equivalent SQL query, assuming $E$ has schema \texttt{(head, tail)}, is the following:
\begin{lstlisting}[language=SQL]
SELECT E1.head, E2.tail
  FROM E AS E1, E AS E2
 WHERE E1.tail = E2.head
\end{lstlisting}
\end{ex}

Finally, a set of multiple conjunctive queries with the same head relation
 defines a {\em union of conjunctive queries} (UCQ).
It has the form:
\begin{align*}
  Q(\bm{X}) & \cd R_{i_1}(\bm{X}_1), R_{i_2}(\bm{X}_2), \ldots, R_{i_m}(\bm{X}_m) \\
  Q(\bm{Y}) & \cd R_{j_1}(\bm{Y}_1), R_{j_2}(\bm{Y}_2), \ldots, R_{j_n}(\bm{Y}_n) \\
  & \hspace{4em} \cdots
\end{align*}
As the name suggests, a UCQ computes the union of each conjunctive query.

\section{Relational Algebra}
\label{sec:join}

Relational queries are compiled to relational algebra 
 before they can be executed by a database system.
The relational join is the central operation in relational algebra,
 as it connects data from different relations 
 and composes computation from different queries.
In this section we describe the semantics of the join
 as well as a simple algorithm to compute it.
We also review other relational algebra operators,
 namely selection, projection, and union.

A {\em full} conjunctive query $Q$ where 
 no atom contains constants or duplicate variables
 is called a {\em natural join} over the relations in $Q$.
We will only consider the natural join in this thesis, 
 and will simply call it {\em join}.
A {\em binary} join is a join involving only two relations.
We can compute the natural join of multiple relations 
 by joining two relations at a time.
Given a conjunctive $Q(\bm{X}) \cd R_1(\bm{X}_1), R_2(\bm{X}_2), \ldots, R_n(\bm{X}_n)$,
 define the following queries:
\begin{align*}
Q_2({\bm X_1} \cup {\bm X_2}) &\cd R_1({\bm X_1}), R_2({\bm X_2})\\
Q_3({\bm X_1} \cup {\bm X_2} \cup {\bm X_3}) &\cd Q_2({\bm X_1} \cup {\bm X_2}), R_3({\bm X_3})\\
& \vdots \\
Q_n({\bm X_1} \cup {\bm X_2} \cup \cdots \cup {\bm X_n}) &\cd Q_{n-1}({\bm X_1} \cup {\bm X_2} \cup \cdots \cup {\bm X_{n-1}}), R_n({\bm X_n})
\end{align*}
We abuse ``${\bm X_1} \cup {\bm X_2}$'' to mean a tuple containing 
 all variables in ${\bm X_1}$ and ${\bm X_2}$ without duplicates, in any order, 
 and similarly for ``${\bm X_1} \cap {\bm X_2}$''.
It is easy to check that $Q_n = Q$. 
Indeed, most modern database systems compute the natural join of multiple relations 
 by joining two relations at a time, 
 after reordering the binary joins carefully for efficiency. 

A simple yet effective algorithm to compute the binary join is called {\em hash join},
 and it is implemented in nearly every database system.
Algorithm~\ref{algo:hash-join} shows the pseudocode for hash join.
Given a full conjunctive query (without constants) 
 of two relations $Q({\bm X_1} \cup {\bm X_2}) \cd R({\bm X_1}), S({\bm X_2})$,
 let ${\bm X} \defeq {\bm X_1} \cap {\bm X_2}$.
%  and let ${\bm i}$ be the positions of each ${\bm x}$ in ${\bm x_2}$.
We pick one of the relations to be the {\em left} relation and the other to be the {\em right} relation.
Suppose we pick $R$ to be the left relation, and $S$ to be the right relation.
We first initialize $s$ to be a empty hash map
 where each key will be (a portion of) a tuple in $S$,
 mapped to a vector of tuples in $S$.
Then we iterate over each tuple in $S$, 
 binding the values to ${\bm X_2}$.
In each iteration,
 we insert ${\bm X_2}$ to the vector mapped to by the key ${\bm X}$ (recall ${\bm X}$ contains a subset of the variables in ${\bm X_2}$).
Next, we iterate over each tuple in $R$,
 binding the values to ${\bm X_1}$,
 and look up the hash map to find all tuples in $s$ that match the values in ${\bm X}$.
For each match, we concatenate the tuples from each relation,
 and output the result.
%
\begin{algorithm}[th]
    $s \gets$ new hash map\;
    \For { ${\bm X_2} \in S$ }
    {
        $s$[${\bm X}$].insert(${\bm X_2}$)\;
    }
    \For { ${\bm X_1} \in R$ }
    {
        \For { ${\bm X_2}$ $\in$ $s[{\bm X}]$ }
        {
            output $({\bm X_1}\cup {\bm X_2})$\;
        }
    }
    \caption{Hash join of $R({\bm X_1})$ and $S({\bm X_2})$, using $S$ as the right relation.}
    \label{algo:hash-join}
\end{algorithm}

To support conjunctive queries with constants, 
 duplicate variables, and those that are not full,
 we need two additional operators from relational algebra, 
 namely selection ($\sigma$) and projection ($\pi$).
The selection operator $\sigma$ takes as arguments an atom $R({\bm X})$
 and a predicate $\theta$ over ${\bm X}$,
 and returns a subset of the relation $R$ containing only the tuples that satisfy $\theta$.
For example, $\sigma_{X = 1}E(X, Y)$ returns all edges in $E$ that start at vertex 1.
For another example, $\sigma_{X = Y}E(X, Y)$ returns all edges in $E$ that are self-loops;
 it is equivalent to repeating the variable $X$, i.e., $E(X, X)$.
The projection operator $\pi$ takes as arguments an atom $R({\bm X})$
 and a set of variables ${\bm Y} \subseteq {\bm X}$,
 and returns a relation containing only attributes corresponding 
 to the variables in ${\bm Y}$.
For example, $\pi_{X}E(X, Y)$ returns all source vertices in $E$.
Both selection and projection can be implemented with a simple loop over the tuples in the input relation.
Finally, computing UCQs requires the union operator $\cup$,
 which simply takes the set union of the relations.
Popular databases also implement several additional relational algebra operators 
 such as aggregation.
In Chapter~\ref{chap:datalogo} we will show how to support aggregation
 as a generalization of projection, 
 when we extend the relational algebra with semirings.

\begin{ex}
\label{ex:selection}
To compute the query $Q(X) \cd E(X, Y), E(Y, X)$ 
 (set of vertices that loop back to themselves in two steps),
 we first compute the join $J(X, Y, Z) \cd E(X, Y), E(Y, Z)$, 
 then select the tuples where $X = Z$ with $S(X, Y, Z) \cd \sigma_{X=Z} J(X, Y, Z)$.
 then project out $Y$ and $Z$ with $Q(X) \cd \pi_{X} S(X, Y, Z)$.
\end{ex}
 
Although we have presented the evaluation of a query 
 by computing a sequence of intermediate queries, 
 like $J$ and $S$ in Example~\ref{ex:selection},
 in practice the database system will combine
 the different relational algebra operators 
 and compute the result ``in one go''.
This approach to avoid the intermediates 
 is called {\em pipelining}.
For example, to compute $Q$ in Example~\ref{ex:selection},
 we may fuse together the inner loop of the join
 with the loops of the selection and projection, 
 as shown in Algorithm~\ref{algo:pipeline}.

\begin{algorithm}
$e \gets $ new hash map \;
\For { $(Y, Z) \in E$ }
{
    $e[Y].insert((Y, Z))$\;
}
\For { $(X, Y) \in E$ }
{
    \For { $(Y, Z) \in e[Y]$ }
    {
        \If { $X = Z$ }
        {
            output $X$\;
        }
    }
}
\caption{Pipelined execution of $Q$ in Example~\ref{ex:selection}.}
\label{algo:pipeline} 
\end{algorithm}

\section{Datalog}
\label{sec:datalog}

Many applications in modern data analytics require recursion.
For example, we may want to detect unreachable nodes in a network.
This can be achieved by computing the transitive closure 
 of the edges in the network,
 which can be expressed as a recursive query.
Datalog is a query language designed to support recursion, 
 and we review its semantics and exeuction in this section.

A Datalog program is a set of conjunctive queries.
Each conjunctive query is also called a {\em rule}, 
 where different rules may share the same head atom. 
If a relation never appears in any head atom,
 we call it an input relation or an {\em extensional database} (EDB) relation.
Otherwise, we call it an output relation or an {\em intensional database} (IDB) relation.

\begin{ex}
\label{ex:datalog}
The following Datalog program computes the transitive closure of the relation $E$. 
\begin{align*}
  &P(X, Y) \cd E(X, Y).\\
  &P(X, Z) \cd P(X, Y), E(Y, Z).
\end{align*}
\end{ex}

The meaning of a Datalog program can be defined as the solution (or {\em model}) of 
 a set of logical constraints.
This is known as the {\em model theoretic semantics} of Datalog.
Specifically, we interpret each atom as a predicate,
 and treat each rule as a $\forall$-quantified logical implication
 from the conjunction of the body atoms to the head atom.
For example, the second rule in Example~\ref{ex:datalog} 
 is interpreted as $\forall X, Y, Z : P(X, Y), E(Y, Z) \implies P(X, Z)$.
Then the result of the Datalog program is the {\em smallest} relations 
 such that all constraints are satisfied.
The emphasis on {\em smallest} is important, 
 because otherwise we may simply set every relation $R$ with arity $k$ 
 to contain all tuples $\setof{(t_1, t_2, \ldots, t_k)}{t_i \in D \text{ for each } i}$
 to satisfy the constraints.
% In fact, an SMT solver would return such a solution if given our constraints.

\begin{ex}
\label{ex:tc}
Setting $E \defeq \set{(1, 2), (2, 3), (3, 4), (4, 5)}$,
 the Datalog program in Example~\ref{ex:datalog}
 computes the set of all paths in the graph $E$, 
 namely $P = \setof{(x, y)}{1 \leq x < y \leq 5}$. 
\end{ex}

An alternative semantics for Datalog is the {\em fixpoint semantics}.
It is equivalent to the model theoretic semantics,
 but also tells us how to evaluate the Datalog program.
For each IDB relation $R_i$, the set of all rules with $R_i$ in the head 
 form a union of conjunctive queries.
This UCQ defines a function $F_i$ from the relations in the rule bodies to $R_i$,
 which we can extend to be a function from all IDB relations to $R_i$ 
 (relations not in the bodies are simply ignored). 
We write $F$ for the set of all $F_i$, 
 and $F$ is then a function from all IDB relations to all IDB relations.
We call $F$ the immediate consequence operator (ICO) of the Datalog program.
To evaluate the Datalog program, as shown in Algorithm~\ref{algo:fixpoint},
 we initialize all IDB relations to be empty,
 then repeatedly apply $F$ to update them, 
 until they no longer change.
This is also known as the {\em na\"ive evaluation} of the Datalog program.
\begin{algorithm}
$I \gets \emptyset$\;
\While { $I$ changes }
{
    $I \gets F(I)$\;
}
\Return{$I$}
\caption{Na\"ive evaluation of a Datalog program. $I$ is the set of IDB relations.}
\label{algo:fixpoint}
\end{algorithm}

\begin{ex}
\label{ex:tc-fixpoint}
The evaluation of the program in Example~\ref{ex:datalog} is shown here:
%
\begin{align*}
    &
    \begin{array}{l c } 
                       & P \\ 
        \text{iter } 1 & \set{(1, 2), (2, 3), (3, 4), (4, 5)} \\ 
        \text{iter } 2 & P^{(1)} \cup \set{ (1, 3), (2, 4), (3, 5)} \\
        \text{iter } 3 & P^{(2)} \cup \set{ (1, 4), (2, 5) } \\
        \text{iter } 4 & P^{(3)} \cup \set{ (1, 5) } \\
        \text{iter } 5 & P^{(4)} \\
    \end{array}
\end{align*}
%
Here we use $P^{(i)}$ to mean the content of relation $P$ at iteration $i$.
Note that in each iteration, each relation contains all tuples from the previous iteration.
This is always true for any Datalog program, 
 and we say that Datalog evaluation is {\em inflationary}.
\end{ex}


An important property of Datalog is that the na\"ive evaluation 
 always terminates, and it does so in polynomial time.
However, the na\"ive evaluation is inefficient in practice.
As shown in Example~\ref{ex:tc-fixpoint},
 each iteration computes the relations from scratch, 
 even though most tuples are already in the relations from the previous iteration.
A more efficient algorithm, the {\em semi-na\"ive evaluation},
 computes only the new tuples in each iteration.
It works by tracking a {\em delta relation} for each relation,
 which contains the new tuples in each iteration.
To compute a delta relation, 
 every rule in the Datalog program is replaced with a set of delta rules.
Given a rule containing $n$ IDB atoms ($R_1$ to $R_n$) and $m$ EDB atoms ($S_1$ to $S_m$):
\[
  R({\bm X}) \cd R_1({\bm X}_1), R_2({\bm X}_2), \ldots, R_n({\bm X}_n), S_1({\bm Y}_1), S_2({\bm Y}_2), \ldots, S_m({\bm Y}_m).
\]
we generate $n$ delta rules, where the $i$-th rule has a delta relation for the $i$-th IDB atom,
 and all EDB atoms are unchanged:
%
\begin{align*}
\Delta R({\bm X}) &\cd \Delta R_1({\bm X}_1)\color{gray}, R_2({\bm X}_2), \ldots, R_n({\bm X}_n), S_1({\bm Y}_1), S_2({\bm Y}_2), \ldots, S_m({\bm Y}_m). \\
\Delta R({\bm X}) &\cd {\color{gray} R_1({\bm X}_1),} \Delta R_2({\bm X}_2)\color{gray}, \ldots, R_n({\bm X}_n), S_1({\bm Y}_1), S_2({\bm Y}_2), \ldots, S_m({\bm Y}_m). \\
\Delta R({\bm X}) &\cd {\color{gray} R_1({\bm X}_1), R_2({\bm X}_2), \ldots,} \Delta R_n({\bm X}_n)\color{gray}, S_1({\bm Y}_1), S_2({\bm Y}_2), \ldots, S_m({\bm Y}_m). \\
\end{align*}
%
Taking these delta rules, we define a new ``delta ICO'' for the program which we denote $\delta F$.
Given IDB relations $I$ and their delta relations $\Delta I$,
 $\delta F(I, \Delta I)$ computes the new delta relations.
The semi-na\"ive evaluation then proceeds as shown in Algorithm~\ref{algo:semi-naive}.
%
\begin{algorithm}
$I \gets F(\emptyset)$, $\Delta I \gets F(\emptyset)$ \label{algo:sn:init} \;
\While { $\Delta I \neq \emptyset$ }
{
    $\Delta I \gets \delta F(I, \Delta I) - I$\;
    $I \gets I \cup \Delta I$
}
\Return{$I$}
\caption{Semi-na\"ive evaluation of a Datalog program.}
\label{algo:semi-naive}
\end{algorithm}
%
In each iteration, we apply the delta rules to compute the delta relations $\Delta I$,
 while taking care to remove already-discovered tuples in $I$ from $\Delta I$.
We take the union of $\Delta I$ and the previous $I$ to compute the new $I$,
 and stop when $\Delta I$ is empty. 

\begin{ex}
\label{ex:tc-sn}
The delta rules for the program in Example~\ref{ex:datalog} are as follows:
\begin{align*}
\Delta P(X, Y) &\cd E(X, Y). \\
\Delta P(X, Z) &\cd \Delta P(X, Y), E(Y, Z).
\end{align*}
The execution of semi-na\"ive evaluation is shown here:
%
\begin{align*}
    &
    \begin{array}{l c c} 
                       & \Delta P & P \\ 
        \text{iter } 1 & \set{(1, 2), (2, 3), (3, 4), (4, 5)} & \Delta P^{(1)} \\
        \text{iter } 2 & \set{(1, 3), (2, 4), (3, 5)} & P^{(1)} \cup \Delta P^{(2)} \\
        \text{iter } 3 & \set{(1, 4), (2, 5)} & P^{(2)} \cup \Delta P^{(3)} \\
        \text{iter } 4 & \set{(1, 5)} & P^{(3)} \cup \Delta P^{(4)} \\
        \text{iter } 5 & \emptyset & P^{(4)} 
    \end{array}
\end{align*}
%
For consistency with the na\"ive algorithm, we count the initialization code in line~\ref{algo:sn:init} 
 of Algorithm~\ref{algo:semi-naive} as ``iteration 1''.
In every iteration, although we compute the same $P$ relation as the na\"ive algorithm,
 we only need to join $\Delta P$ with $E$.
Because $\Delta P$ gets smaller as time goes on, 
 every iteration runs faster in the semi-na\"ive algorithm.
\end{ex}