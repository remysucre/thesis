\chapter{Related Work}
\label{chap:related}

In this chapter we cover related work to this dissertation.
First, we discuss prior proposals for relational programming languages.
Then we review the literature on join algorithms, with an emphasis on multi-way joins.
Finally, we cover existing techniques for optimizing relational programs.

\section{Relational Programming Languages}
\label{sec:related:relational}

We are not the first to use the term ``relational programming''.
It appeared as early as 1981 in a paper by Bruce J. MacLennan~\cite{maclennan1981introduction}, 
 where it is described as ``a style of programming in which entire relations 
 are manipulated rather than individual data''.
The term was also used as a synonym for logic programming a la Prolog~\cite{colmerauer1990introduction}, 
 where the fundamental building blocks are relations.
While there is, to the best of our knowledge, no surviving implementation of MacLennan's relational programming language, 
 many Prolog systems are still being used and actively 
 developed~\cite{zhou2012language, bueno1997ciao, diaz2001design, prolog2021scryer, wielemaker2012swi}.
Both MacLennan and proponents of Prolog emphasize one aspect of their languages: 
 thanks to a relational foundation, the programmer can specify 
 complex logic at a very high level, resulting in concise and elegant programs.
As a result, such langauges found success in classic AI applications
 like expert systems, which require sophisticated symbolic reasoning~\cite{korner2022fifty}.

More recently, a class of languages called miniKanren has renewed interest 
 in relational programming~\cite{byrd2009relational, byrd2012minikanren, rozplokhas2019certified, DBLP:books/daglib/0015651}.
The designers of miniKanren were motivated by the ``impurity'' of Prolog: 
 since Prolog evaluation is based on backtracking, 
 Prolog programmers frequently resort to imperative features like the ``cut'' operator 
 to prune the enormous search space.
In contrast, miniKanren is ``purely logical'' or ``purely relational'', 
 in that it more strictly adheres to the declarative paradigm.
Another distinguishing feature of miniKanren is its simple design: 
 the core language consists of only three logical operators: 
 the equality constraint \verb|==|, disjunction \verb|conde|, 
 and variable introduction \verb|fresh|.
Thanks to its simplicity, miniKanren has been embedded in dozens of programming languages. 

A closer line of work to us centers around the Datalog language.
We will cover Datalog in more detail in Chapter~\ref{chap:background}, 
 but note here that it is a ``pure'' subset of Prolog.
There are well-established declarative semantics for Datalog based on proof theory 
 and model theory~\cite{DBLP:books/aw/AbiteboulHV95}.
As we will see, Datalog is also delightfully simple: 
 a Datalog program is just a set of inference rules, 
 and the na\"ive evaluation algorithm for Datalog just applies the rules in a loop until a fixpoint.
 
Nevertheless, the core Datalog language is very limited. 
It does not allow negation, arithmetic, or aggregation, 
 all of which are essential for modern data analysis.
Researchers have therefore proposed various extensions to Datalog
 to support such constructs.
In this dissertation, we focus on extensions
 that aim to support {\em aggregates} in Datalog.  These include, but
 are not limited to, the standard \texttt{MIN}, \texttt{MAX},
 \texttt{SUM}, and \texttt{COUNT} aggregates in SQL.
 
 The main challenge in having aggregates is that they are not {\em
   monotone} under set inclusion, yet monotonicity is crucial for the
 declarative semantics of Datalog, and optimizations like semi-na\"ive
 evaluation.  
%  Consider the APSP Example~\ref{ex:apsp}.  We could
%  attempt to write it in Datalog, by extending the language with a
%  $\min$-aggregation:
%  %
%  \begin{align}
%    P(x,y,d) & \cd E(x,y,d) \nonumber\\
%    P(x,y,\min(d)) & \cd P(x,z,d_1), E(z,y,d_2), d=d_1+d_2 \label{eqn:non:monotone}
% \end{align}
%  However, the second rule is not monotone w.r.t.~set inclusion.
%  This is a subtle, but important point.  For example, fix
%  $E = \set{(b,c,20),(b',c,10)}$: if $P$ is $\set{(a,b,1)}$, then the
%  output of the rule is $\set{(a,c,21)}$, but when $P$ is the superset
%  $\set{(a,b,1),(a,b',1)}$, then the output is $\set{(a,c,11)}$, which
%  is not a superset of the previous output,
%  $\set{(a,c,21)}\not\subseteq \set{(a,c,11)}$.
 Approaches to resolve the tension between aggregates and monotonicity
 mainly follow two strategies: break the program into {\em strata}, or
 generalize the order relation to ensure that aggregates become
 monotone.
 
 \paragraph*{\bf Stratified Aggregates}
 The simplest way to add aggregates to Datalog while staying monotone
  is to disallow aggregates in recursion.
 Proposed by Mumick et al.~\cite{DBLP:conf/vldb/MumickPR90}, the idea is inspired by stratified negation,
  where every negated relation must be computed in a previous stratum.
%  Ex.~\ref{ex:apsp-stratified} is stratified:
%   the first two rules form the first stratum and compute $P_{\text{all}}$
%   to a fixpoint in regular Datalog\footnote{The second rule uses the built-in function $+$.},
%   and the last rule performs the $\min$ aggregate in its own stratum.
 Stratifying aggregates has the benefit that the semantics,
  evaluation algorithms, and optimizations for classic Datalog
  can be applied unchanged to each stratum.
 However, stratification limits the programs one is allowed to write --
   in Chapter~\ref{chap:datalogo} we show two versions of the same program, 
   one stratified and one not, yet the latter is more efficient.
 The stratification requirement can also be a cognitive burden on the programmer.
 In fact, the most general notion of stratification, dubbed ``magic stratification''~\cite{DBLP:conf/vldb/MumickPR90}, involves both a syntactic condition and a semantic condition defined
  in terms of derivation trees.
 
 \paragraph*{\bf Generalized Ordering}
 In this dissertation, we follow the approach that restores monotonicity by
 generalizing the ordering on which monotonicity is defined.  The key
 idea is that, although a program is not monotone according to the
 $\subseteq$ ordering on sets, we can pick another order under which
 $P$ \textbf{is} monotone.  Ross and
 Sagiv~\cite{DBLP:conf/pods/RossS92} define the ordering\footnote{If
   $(x, y)$ is not a key in $P$, then $\sqsubseteq$ is only a
   preorder.}  $P \sqsubseteq P'$ as:
 \begin{align*}
     \forall (x, y, d) \in P, \exists (x, y, d') \in P' : (x, y, d) \sqsubseteq (x, y, d') \\
     \text{where } (x, y, d) \sqsubseteq (x', y', d') \defeq x = x' \wedge y = y' \wedge d \geq d'
 \end{align*}
 %%% \dan{If $(x,y)$ is not a key in $P$, then $\sqsubseteq$ is only a
 %%%   preorder.  I wonder whether we should metion this explicitly,
 %%%   justifying the need for $P$ to be a mapping, as described below; I'm
 %%%   just not sure whether we need so much detail.}
 %%% \remy{I added a footnote to clarify this.}
 %%% %
 That is, $P$ increases if we replace $(x, y, d)$ with $(x, y, d')$
 where $d' < d$, for example $\set{(a,c,21)}\sqsubseteq \set{(a,c,11)}$. 
 In general, to define a generalized ordering we need to view a relation as a map
  from a tuple to an element in some ordered set $S$.
 For example, the relation $P$ maps a pair of vertices $(x,y)$ to a distance $d$.
 We will call such generalized relations $S$-relations.
 Different approaches in existing work have modeled $S$ using different algebraic structures:
 Ross and Sagiv~\cite{DBLP:conf/pods/RossS92} require it to be a complete lattice,
  Conway et al.~\cite{DBLP:conf/cloud/ConwayMAHM12} require only a semilattice,
  whereas Green et al.~\cite{DBLP:conf/pods/GreenKT07} require $S$ to be an $\omega$-continuous semiring.
 These proposals bundle the ordering together with two operations $\otimes$ and/or $\oplus$.
 % with $\otimes$ generalizing join to work on $S$-relations, and $\oplus$ generalizing union.
 In this dissertation, we follow this line of work and ensure monotonicity by generalizing the ordering.
 However, in contrast to prior work we {\em decouple} operations on $S$-relations from the ordering,
 and allow one to freely mix and match the two as long as monotonicity is respected.
 
 \paragraph*{\bf Other Approaches}
 There are other approaches to support aggregates in Datalog that do not fall into
  the two categories above. We highlight a few of them here.
  Ganguly et al.~\cite{DBLP:conf/pods/GangulyGZ91}
  model $\min$ and $\max$ aggregates in Datalog with negation,
  thereby supporting aggregates via semantics defined for negation.
  Mazuran et al.~\cite{DBLP:journals/tplp/MazuranSZ13}
  extend Datalog with counting quantification,
  which additionally captures \texttt{SUM}.
 Kemp and Stuckey~\cite{DBLP:conf/slp/KempS91} extend the well-founded semantics~\cite{DBLP:journals/jacm/GelderRS91, DBLP:conf/pods/Gelder89}
  and stable model semantics~\cite{DBLP:conf/iclp/GelfondL88} of Datalog
  to support recursive aggregates.
 They show that their semantics captures various previous attempts to incorporate aggregates into Datalog.
 They also discuss a semantics using {\em closed} semirings, and observe that under such a semantics
  some programs may not have a unique stable model.
 Semantics of aggregation in Answer Set Programming has been extensively studied
  \cite{DBLP:journals/ai/FaberPL11, DBLP:journals/tplp/GelfondZ14, DBLP:journals/fuin/Alviano16}.
 Liu and Stoller~\cite{DBLP:conf/lfcs/LiuS22} give a comprehensive survey of this space.

\section{Join Algorithms}
\label{sec:related:joins}

Join algorithms have been studied since the birth of relational databases, 
 and there are thousands of papers focusing on improving the performance of joins.
Algorithms for computing the join roughly fall into two categories: 
 binary joins and multi-way joins.
We will provide a more in-depth comparison of the two paradigms in Chapter~\ref{chap:free-join}, 
 and highlight a few distinguishing features here.

Compared to multi-way joins, binary joins are by far more frequently implemented 
 in practice~\cite{gaffney2022sqlite, stonebraker1991postgres, raasveldt2019duckdb}.
This is largely because the former is more modular: 
 to compute the join of multiple relations, 
 we can chain together a sequence of binary joins.
Modularity simplifies implementation, 
 and also makes it easy to optimize, parallelize, incrementalize, and distribute 
 the query, or make it more robust to failures.

On the other hand, multi-way joins can be more efficient than binary joins~\cite{DBLP:conf/vldb/GraefeBC98, DBLP:conf/vldb/KemperKW99, DBLP:conf/sigmod/HellersteinA00}.
By processing multiple relations at the same time, 
 multi-way joins can avoid materializing intermediate results. 
It also has the freedom to reorder the relations at runtime,
 taking advantage of better statistics computed during execution.
More recently, researchers have shown a kind of multi-way join algorithms to be 
 {\em Worst-case optimal}~\cite{DBLP:conf/pods/NgoPRR12, DBLP:conf/icdt/Veldhuizen14,
  DBLP:journals/sigmod/NgoRR13, DBLP:conf/pods/000118},
 making them the algorithms to have stronger asymptotic guarantees than the classic binary join algorithms.

Given that each paradigm has its own strength, 
 researchers have proposed to combine the two within the same 
 system~\cite{DBLP:journals/pvldb/FreitagBSKN20,DBLP:journals/tods/AbergerLTNOR17, DBLP:journals/pvldb/MhedhbiS19}, 
 much like how modern database systems implement both hash joins and sort-merge joins.
We will discuss these systems in more detail in Chapter~\ref{chap:free-join}.
Our approach is motivated by the same idea of bringing the best of both worlds, 
 but instead of implementing both binary join and multi-way join,
 we unify and generalize them into a single join algorithm.

\section{Optimizing Relational Programs}
\label{sec:related:linear-algebra}

There is a vast body of literature for both relational query optimization and
optimizing compilers for machine learning. Since we optimize machine learning programs through a
relational lens, our work relates to research in both fields. 
Numerous state-of-the-art optimizing compilers for machine learning
 resort to syntactic rewrites and heuristics to optimize linear algebra
expressions~\cite{DBLP:reference/bdt/Boehm19}~\cite{DBLP:conf/icml/SujeethLBRCWAOO11}~\cite{DBLP:conf/sigmod/HuangB013}. 
We perform optimization based on a
relational semantics of linear algebra and holistically explore the complex
search space. A majority of relational query optimization focus on join order
optimization \cite{Graefe95a} \cite{MoerkotteN06} \cite{MoerkotteN08}
\cite{selinger1979access}; 
we optimize programs with
join (product), union (addition), and aggregate (sum) operations. Sum-product optimization considers operators other than join while optimizing
relational queries. Recent years have seen a line of excellent theoretical and
practical research in this area \cite{KhamisNR16} \cite{Joglekar2016AJARAA}.
These work gives significant improvement for queries involving $\times$ and $\sum$,
but fall short of LA workloads that occur in practice. We step past these
frameworks by incorporating common subexpression elimination and incorporating addition
($+$). 

% In terms of approach, our design of relational IR ties in to research that
% explores the connection between linear algebra and relational algebra. Our
% design and implementation of the optimizer ties into research that leverage
% equality saturation and \textsc{and-or dag}s for query optimization and compiler
% optimization for programming languages. Since we focus on optimizing sum-product
% expressions in linear algebra, our work naturally relates to research in sum-product
% optimization. We now discuss these three lines of research in detail. 

\subsection{Relational Algebra and Linear Algebra}
Elgamal et al. \cite{ElgamalLBETRS17} envisions \textsc{spoof}, a compiler for machine learning programs
that leverages relational query optimization techniques for LA sum-product optimization. 
We realize this vision by providing the translation
rules from LA to RA and the relational equality rules that completely represents
the search space for sum-product expressions. One important distinction is, Elgamal
et al. proposes \emph{restricted relational algebra} where every expression must
have at most two free attributes. This ensures every relational expression in every step of the
optimization process to be expressible in LA. In contrast, we remove this restriction and only require the 
optimized output to be in linear algebra. This allows us to trek out to spaces not
covered by linear algebra equality rules and achieve completeness. In addition 
to sum-product expressions, Elgamal et al. also considers selection and projection operations
like selecting the positive entries of a matrix. We plan to explore supporting
selection and projection in the future. Elgamal et al. also proposes compile-time generation of
fused operators, which is implemented by Boehm et al.~\cite{DBLP:journals/pvldb/BoehmRHSEP18}.
SPORES can replace operations with existing fused operators when it is beneficial, and we
plan to explore combining sum-product rewrite with fusion generation in the future. 

MorpheusFI by Li et al.~\cite{DBLP:conf/sigmod/LiC019} and LARA by Hutchison et al.~\cite{DBLP:journals/corr/HutchisonHS17} explore optimizations across the interface of machine learning and database systems. In particular, MorpheusFI speeds up machine learning algorithms over large joins by pushing computation into each joined table, thereby avoiding expensive
materialization. LARA implements linear algebra operations with relational operations and
shows competitive optimizations alongside popular data processing systems. 
Schleich et al.\cite{DBLP:conf/sigmod/SchleichOC16} and Khamis et al.\cite{DBLP:journals/corr/NgoNOS17} explore in-database learning, which aims to push entire
machine learning algorithms into the database system. 
We contribute in this space by showing that even without a relational engine, the
relational abstraction can still benefit machine learning tasks as a powerful
intermediate abstraction. \rvt{Kotlyar et.al. \cite{DBLP:conf/europar/KotlyarPS97}
explore compiling sparse linear algebra via
a relational abstraction. We contribute by providing a simple set of rewrite rules
and prove them complete.}

\subsection{Low-level Code Generation}
\rvt{
Novel machine learning compilers including TACO \cite{DBLP:journals/pacmpl/KjolstadKCLA17},
TVM \cite{DBLP:conf/nips/ChenZYJMCGK18}, TensorComprehension \cite{DBLP:journals/corr/abs-1802-04730},
Tiramisu \cite{DBLP:conf/cgo/BaghdadiRRSAZSK19}, and COMET~\cite{DBLP:conf/llvmhpc/TianGLRK21}
 generate efficient low-level code
for kernel operators. These kernels are small expressions that consist of a few
operators. For example the MATTRANSMUL kernel in TACO implements $\alpha A^T x + \beta z$.
The kernels are of interest because they commonly appear in machine learning programs,
and generating efficient low-level implementation for them can greatly impact performance. 
However, these compilers cannot perform algebraic rewrite on large programs as \sys\ does.
For example, TACO
supports only plus, multiply and aggregate, whereas \sys\ supports any custom functions
as discussed in Section~\ref{udfs}; Tiramisu requires tens of lines of code just to
specify matrix multiply which is a single operator in \sys. Furthermore, the basic polyhedral
model in Tiramisu and TensorComprehension does not support sparse matrices. Sparse extensions exist, but require the
user to make subtle tradeoffs between expressivity and performance \cite{DBLP:journals/pieee/StroutHO18}.
At a high level,
we view these kernel compilers as complementary to \sys. The former can provide efficient kernel
implementation just like the fused operators in SystemML, and we can easily include these
kernels in \sys\ for whole-program rewrite. The TASO compiler \cite{DBLP:conf/sosp/JiaPTWZA19}
combines kernel-level rewrite with whole-program rewrite, and is
also driven by a set of equality rules like \sys. However, it induces significant overhead --
generating operator graphs with just up to 4 operators takes 5 minutes, and while \cite{DBLP:conf/sosp/JiaPTWZA19}
does not include detailed time for compiling whole programs, it reports the compilation
finishes in ``less than ten minutes''. In contrast, \sys\ takes seconds instead of minutes
in compilation.} 

% \subsection{Exploiting Sparsity}
% A number of optimizing compilers for linear algebra programs support sparse
% matrix operators . SystemML \cite{boehm2014systemml} and Cumulon
% \cite{DBLP:conf/sigmod/HuangB013} includes a variety of hand-fused sparse operators.
% \cite{ElgamalLBETRS17} further automates the generation of fused operator
% implementation. However, all existing approach either work on a per-operator
% basis, or rely on hand-coded rules to rewrite expressions to fused operators.
% Using equality saturation, we can automatically discover opportunity for
% operator fusion based on a simple set of equality rules. And even with the
% absence of operator fusion, we can still take advantage of sparsity in our
% optimization.

% \subsection{Multi-query Optimization}
% The research in Multi-query Optimization aims to share common subqueries among
% multiple queries to speed up processing \cite{Kathuria017}. This is closely
% related to our problem of exploiting CSEs in linear algebra, although our focus
% is intra-query instead of inter-query. Moreover, the majority of multi-query
% optimizers focus on joins, whereas union and aggregate are essential for linear
% algebra programs. One lesson we can learn from the literature multi-query
% optimization is to perform equality saturation for multiple expressions in the
% same egraph, thereby enabling sharing across optimization sessions and reducing
% compilation time.

\subsection{Recursive Program Optimization and Program Synthesis}
\label{sec:related:recursion}

Our work on optimizing recursive programs was partially inspired by the \prem\
condition, described by Zaniolo et
al.~\cite{DBLP:journals/tplp/ZanioloYDSCI17}, which, as we shall
explain, is a special case of the FGH-rule.  Unlike our system, their
implementation required the programmer to check the \prem\ manually,
then perform the corresponding optimization.
% As we shall discuss, our FGH-rule generalizes \prem.
% Their implementation also relies on the programmer to perform the rewrite and
% check that \prem\ is applicable.
% Our optimizer is automated and supports the more general FGH-rule.
%
Seveal prior systems  leveraged SMT-solvers to reason about
query languages~\cite{
  DBLP:conf/icfem/VeanesGHT09,
  DBLP:conf/cidr/ChuWWC17,
  DBLP:conf/cav/GrossmanCIRS17,
  DBLP:conf/sosp/SchlaipferRLS17,
  DBLP:journals/pacmpl/0001DLC18};
but none of these  consider recursive queries.
% We contribute by proposing a simple and sound SMT encoding that can
% be used in \cegis, and works in conjunction with a rewrite system
% to reason about semiring operations including unbounded aggregation.
%
Datalog synthesizers have been described in~\cite{
  DBLP:conf/cp/AlbarghouthiKNS17,
  DBLP:conf/sigsoft/SiLZAKN18,
  DBLP:conf/ijcai/SiRHN19,
  DBLP:journals/pvldb/WangSCPD20,
  DBLP:journals/pacmpl/RaghothamanMZNS20}.
Their setting is different from ours:
the specification is given by input-output examples, and the
synthesizer needs to produce a program that matches all examples.
% In comparison, we take a Datalog program as the reference implementation
% which completely specifies the semantics of the program to be synthesized.
% Unlike prior work which must synthesize recursive definitions,
% we leverage the FGH-rule to solve the optimization problem
% by synthesizing a non-recursive expression.
A design choice that we made, and which sets us further aside from the
previous systems, is to use an existing \cegis\ system, Rosette; thus,
we do not aim to  improve the \cegis\ system itself, but optimize the
way we use it.
